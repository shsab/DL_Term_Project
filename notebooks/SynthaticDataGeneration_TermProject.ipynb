{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SynthaticDataGeneration-TermProject.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dKDL-TZazZc1"
      ],
      "authorship_tag": "ABX9TyMm0SozPGxn8V04G/oyINAL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shsab/DL_Term_Project/blob/main/notebooks/SynthaticDataGeneration_TermProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru0feLyYzcLD"
      },
      "source": [
        "# Synthetic Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RETaw2K8zi8a"
      },
      "source": [
        "## TimeGAN implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imnU5Dd9zq6S"
      },
      "source": [
        "### Initialization and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQtxfrmjzpyC"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow import function\n",
        "\n",
        "from joblib import dump, load\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWYZD6jI2mmI"
      },
      "source": [
        "### Utility methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FYs7ZDVHBAH"
      },
      "source": [
        "def construct_network(model, n_layers, hidden_units, output_units, net_type='GRU'):\n",
        "    if net_type=='GRU':\n",
        "        for i in range(n_layers):\n",
        "            model.add(keras.layers.GRU(units=hidden_units,\n",
        "                                       return_sequences=True,\n",
        "                                       name=f'GRU_{i + 1}'))\n",
        "    else:\n",
        "        for i in range(n_layers):\n",
        "            model.add(keras.layers.LSTM(units=hidden_units,\n",
        "                                        return_sequences=True,\n",
        "                                        name=f'LSTM_{i + 1}'))\n",
        "\n",
        "    model.add(keras.layers.Dense(units=output_units,\n",
        "                                 activation='sigmoid',\n",
        "                                 name='OUT'))\n",
        "    return model\n",
        "\n",
        "\n",
        "def unpack(model, training_config, weights):\n",
        "    restored_model = keras.layers.deserialize(model)\n",
        "    if training_config is not None:\n",
        "        restored_model.compile(**keras.saving.saving_utils.compile_args_from_training_config(training_config))\n",
        "    restored_model.set_weights(weights)\n",
        "    return restored_model\n",
        "\n",
        "\n",
        "def make_keras_picklable():\n",
        "    def __reduce__(self):\n",
        "        _metadata = keras.saving.saving_utils.model_metadata(self)\n",
        "        training_config = _metadata.get(\"training_config\", None)\n",
        "        model = keras.layers.serialize(self)\n",
        "        model_weights = self.get_weights()\n",
        "        return (unpack, (model, training_config, model_weights))\n",
        "\n",
        "    cls = keras.Model\n",
        "    cls.__reduce__=__reduce__"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKDL-TZazZc1"
      },
      "source": [
        "### Absolute GAN implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJcuTpCQdEkL"
      },
      "source": [
        "class GanModel():\n",
        "    def __init__(self, model_parameters):\n",
        "        gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "        if len(gpu_devices) > 0:\n",
        "            try:\n",
        "                tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "            except:\n",
        "                # Invalid device or cannot modify virtual devices once initialized.\n",
        "                pass\n",
        "\n",
        "        self._model_parameters = model_parameters\n",
        "        [self.batch_size,\n",
        "         self.lr,\n",
        "         self.beta_1,\n",
        "         self.beta_2,\n",
        "         self.noise_dim,\n",
        "         self.data_dim,\n",
        "         self.layers_dim] = model_parameters\n",
        "        self.define_gan()\n",
        "\n",
        "    def __call__(self, inputs, **kwargs):\n",
        "        return self.model(inputs=inputs, **kwargs)\n",
        "\n",
        "    def define_gan(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self, network):\n",
        "        return network.trainable_variables\n",
        "\n",
        "    @property\n",
        "    def model_parameters(self):\n",
        "        return self._model_parameters\n",
        "\n",
        "    @property\n",
        "    def model_name(self):\n",
        "        return self.__class__.__name__\n",
        "\n",
        "    def train(self, data, train_arguments):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def sample(self, n_samples):\n",
        "        steps = n_samples // self.batch_size + 1\n",
        "        data = []\n",
        "        for _ in tqdm.trange(steps, desc='Synthetic data generation'):\n",
        "            z = tf.random.uniform([self.batch_size, self.noise_dim])\n",
        "            records = tf.make_ndarray(tf.make_tensor_proto(self.generator(z, training=False)))\n",
        "            data.append(pd.DataFrame(records))\n",
        "        return pd.concat(data)\n",
        "\n",
        "    def save(self, path):\n",
        "        make_keras_picklable()\n",
        "        try:\n",
        "            dump(self, path)\n",
        "        except:\n",
        "            raise Exception('Please provide a valid path to save the model.')\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "        if len(gpu_devices) > 0:\n",
        "            try:\n",
        "                tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "            except:\n",
        "                # Invalid device or cannot modify virtual devices once initialized.\n",
        "                pass\n",
        "        synth = load(path)\n",
        "        return synth"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0-JJPAi7P41"
      },
      "source": [
        "### Generator implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_BKFOa87VlS"
      },
      "source": [
        "class Generator(keras.Model):\n",
        "    def __init__(self, hidden_dim, net_type='GRU'):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.net_type = net_type\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        model = keras.Sequential(name='Generator')\n",
        "        model = construct_network(model,\n",
        "                                  n_layers=3,\n",
        "                                  hidden_units=self.hidden_dim,\n",
        "                                  output_units=self.hidden_dim,\n",
        "                                  net_type=self.net_type)\n",
        "        return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKZVExOS7acq"
      },
      "source": [
        "### Discriminator implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Npa7G97fli"
      },
      "source": [
        "class Discriminator(keras.Model):\n",
        "    def __init__(self, hidden_dim, net_type='GRU'):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.net_type=net_type\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        model = keras.Sequential(name='Discriminator')\n",
        "        model = construct_network(model,\n",
        "                                  n_layers=3,\n",
        "                                  hidden_units=self.hidden_dim,\n",
        "                                  output_units=1,\n",
        "                                  net_type=self.net_type)\n",
        "        return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy0BCKg87k4y"
      },
      "source": [
        "### Recovery implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T-eIUZ87oqe"
      },
      "source": [
        "class Recovery(keras.Model):\n",
        "    def __init__(self, hidden_dim, n_seq):\n",
        "        self.hidden_dim=hidden_dim\n",
        "        self.n_seq=n_seq\n",
        "        return\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        recovery = keras.Sequential(name='Recovery')\n",
        "        recovery = construct_network(recovery,\n",
        "                                     n_layers=3,\n",
        "                                     hidden_units=self.hidden_dim,\n",
        "                                     output_units=self.n_seq)\n",
        "        return recovery"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTCEUQyV7oGI"
      },
      "source": [
        "### Embedder implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-v9HK427vfM"
      },
      "source": [
        "class Embedder(keras.Model):\n",
        "    def __init__(self, hidden_dim):\n",
        "        self.hidden_dim=hidden_dim\n",
        "        return\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        embedder = keras.Sequential(name='Embedder')\n",
        "        embedder = construct_network(embedder,\n",
        "                                     n_layers=3,\n",
        "                                     hidden_units=self.hidden_dim,\n",
        "                                     output_units=self.hidden_dim)\n",
        "        return embedder"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mczc2_9i71Ch"
      },
      "source": [
        "### Supervisor implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmTO6o9h76JN"
      },
      "source": [
        "class Supervisor(keras.Model):\n",
        "    def __init__(self, hidden_dim):\n",
        "        self.hidden_dim=hidden_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        model = keras.Sequential(name='Supervisor')\n",
        "        model = construct_network(model,\n",
        "                                  n_layers=2,\n",
        "                                  hidden_units=self.hidden_dim,\n",
        "                                  output_units=self.hidden_dim)\n",
        "        return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQKIhnBB7Aev"
      },
      "source": [
        "### TimeGAN implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpdcuU5Z7ECw"
      },
      "source": [
        "class TimeGAN(GanModel):\n",
        "    def __init__(self, model_parameters, hidden_dim, seq_len, n_seq, gamma):\n",
        "        self.seq_len=seq_len\n",
        "        self.n_seq=n_seq\n",
        "        self.hidden_dim=hidden_dim\n",
        "        self.gamma=gamma\n",
        "        super().__init__(model_parameters)\n",
        "\n",
        "    def define_gan(self):\n",
        "        self.generator_aux=Generator(self.hidden_dim).build(input_shape=(self.seq_len, self.n_seq))\n",
        "        self.supervisor=Supervisor(self.hidden_dim).build(input_shape=(self.hidden_dim, self.hidden_dim))\n",
        "        self.discriminator=Discriminator(self.hidden_dim).build(input_shape=(self.hidden_dim, self.hidden_dim))\n",
        "        self.recovery = Recovery(self.hidden_dim, self.n_seq).build(input_shape=(self.hidden_dim, self.hidden_dim))\n",
        "        self.embedder = Embedder(self.hidden_dim).build(input_shape=(self.seq_len, self.n_seq))\n",
        "\n",
        "        X = keras.Input(shape=[self.seq_len, self.n_seq], batch_size=self.batch_size, name='RealData')\n",
        "        Z = keras.Input(shape=[self.seq_len, self.n_seq], batch_size=self.batch_size, name='RandomNoise')\n",
        "\n",
        "        # Building the AutoEncoder\n",
        "        H = self.embedder(X)\n",
        "        X_tilde = self.recovery(H)\n",
        "\n",
        "        self.autoencoder = keras.Model(inputs=X, outputs=X_tilde)\n",
        "\n",
        "        # Adversarial Supervise Architecture\n",
        "        E_Hat = self.generator_aux(Z)\n",
        "        H_hat = self.supervisor(E_Hat)\n",
        "        Y_fake = self.discriminator(H_hat)\n",
        "\n",
        "        self.adversarial_supervised = keras.Model(inputs=Z,\n",
        "                                       outputs=Y_fake,\n",
        "                                       name='AdversarialSupervised')\n",
        "\n",
        "        # Adversarial architecture in latent space\n",
        "        Y_fake_e = self.discriminator(E_Hat)\n",
        "\n",
        "        self.adversarial_embedded = keras.Model(inputs=Z,\n",
        "                                    outputs=Y_fake_e,\n",
        "                                    name='AdversarialEmbedded')\n",
        "        # Synthetic data generation\n",
        "        X_hat = self.recovery(H_hat)\n",
        "        self.generator = keras.Model(inputs=Z,\n",
        "                            outputs=X_hat,\n",
        "                            name='FinalGenerator')\n",
        "\n",
        "        # Final discriminator model\n",
        "        Y_real = self.discriminator(H)\n",
        "        self.discriminator_model = keras.Model(inputs=X,\n",
        "                                         outputs=Y_real,\n",
        "                                         name=\"RealDiscriminator\")\n",
        "\n",
        "        # Define the loss functions\n",
        "        self._mse=keras.losses.MeanSquaredError()\n",
        "        self._bce=keras.losses.BinaryCrossentropy()\n",
        "\n",
        "\n",
        "    @function\n",
        "    def train_autoencoder(self, x, opt):\n",
        "        with tf.GradientTape() as tape:\n",
        "            x_tilde = self.autoencoder(x)\n",
        "            embedding_loss_t0 = self._mse(x, x_tilde)\n",
        "            e_loss_0 = 10 * tf.sqrt(embedding_loss_t0)\n",
        "\n",
        "        var_list = self.embedder.trainable_variables + self.recovery.trainable_variables\n",
        "        gradients = tape.gradient(e_loss_0, var_list)\n",
        "        opt.apply_gradients(zip(gradients, var_list))\n",
        "        return tf.sqrt(embedding_loss_t0)\n",
        "\n",
        "    @function\n",
        "    def train_supervisor(self, x, opt):\n",
        "        with tf.GradientTape() as tape:\n",
        "            h = self.embedder(x)\n",
        "            h_hat_supervised = self.supervisor(h)\n",
        "            g_loss_s = self._mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
        "\n",
        "        var_list = self.supervisor.trainable_variables + self.generator.trainable_variables\n",
        "        gradients = tape.gradient(g_loss_s, var_list)\n",
        "        apply_grads = [(grad, var) for (grad, var) in zip(gradients, var_list) if grad is not None]\n",
        "        opt.apply_gradients(apply_grads)\n",
        "        return g_loss_s\n",
        "\n",
        "    @function\n",
        "    def train_embedder(self,x, opt):\n",
        "        with tf.GradientTape() as tape:\n",
        "            h = self.embedder(x)\n",
        "            h_hat_supervised = self.supervisor(h)\n",
        "            generator_loss_supervised = self._mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
        "\n",
        "            x_tilde = self.autoencoder(x)\n",
        "            embedding_loss_t0 = self._mse(x, x_tilde)\n",
        "            e_loss = 10 * tf.sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised\n",
        "\n",
        "        var_list = self.embedder.trainable_variables + self.recovery.trainable_variables\n",
        "        gradients = tape.gradient(e_loss, var_list)\n",
        "        opt.apply_gradients(zip(gradients, var_list))\n",
        "        return tf.sqrt(embedding_loss_t0)\n",
        "\n",
        "    def discriminator_loss(self, x, z):\n",
        "        y_real = self.discriminator_model(x)\n",
        "        discriminator_loss_real = self._bce(y_true=tf.ones_like(y_real),\n",
        "                                            y_pred=y_real)\n",
        "\n",
        "        y_fake = self.adversarial_supervised(z)\n",
        "        discriminator_loss_fake = self._bce(y_true=tf.zeros_like(y_fake),\n",
        "                                            y_pred=y_fake)\n",
        "\n",
        "        y_fake_e = self.adversarial_embedded(z)\n",
        "        discriminator_loss_fake_e = self._bce(y_true=tf.zeros_like(y_fake_e),\n",
        "                                              y_pred=y_fake_e)\n",
        "        return (discriminator_loss_real +\n",
        "                discriminator_loss_fake +\n",
        "                self.gamma * discriminator_loss_fake_e)\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_generator_moments_loss(y_true, y_pred):\n",
        "        y_true_mean, y_true_var = tf.nn.moments(x=y_true, axes=[0])\n",
        "        y_pred_mean, y_pred_var = tf.nn.moments(x=y_pred, axes=[0])\n",
        "        g_loss_mean = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))\n",
        "        g_loss_var = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - tf.sqrt(y_pred_var + 1e-6)))\n",
        "        return g_loss_mean + g_loss_var\n",
        "\n",
        "    @function\n",
        "    def train_generator(self, x, z, opt):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_fake = self.adversarial_supervised(z)\n",
        "            generator_loss_unsupervised = self._bce(y_true=tf.ones_like(y_fake),\n",
        "                                                    y_pred=y_fake)\n",
        "\n",
        "            y_fake_e = self.adversarial_embedded(z)\n",
        "            generator_loss_unsupervised_e = self._bce(y_true=tf.ones_like(y_fake_e),\n",
        "                                                      y_pred=y_fake_e)\n",
        "            h = self.embedder(x)\n",
        "            h_hat_supervised = self.supervisor(h)\n",
        "            generator_loss_supervised = self._mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
        "\n",
        "            x_hat = self.generator(z)\n",
        "            generator_moment_loss = self.calc_generator_moments_loss(x, x_hat)\n",
        "\n",
        "            generator_loss = (generator_loss_unsupervised +\n",
        "                              generator_loss_unsupervised_e +\n",
        "                              100 * tf.sqrt(generator_loss_supervised) +\n",
        "                              100 * generator_moment_loss)\n",
        "\n",
        "        var_list = self.generator_aux.trainable_variables + self.supervisor.trainable_variables\n",
        "        gradients = tape.gradient(generator_loss, var_list)\n",
        "        opt.apply_gradients(zip(gradients, var_list))\n",
        "        return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss\n",
        "\n",
        "    @function\n",
        "    def train_discriminator(self, x, z, opt):\n",
        "        with tf.GradientTape() as tape:\n",
        "            discriminator_loss = self.discriminator_loss(x, z)\n",
        "\n",
        "        var_list = self.discriminator.trainable_variables\n",
        "        gradients = tape.gradient(discriminator_loss, var_list)\n",
        "        opt.apply_gradients(zip(gradients, var_list))\n",
        "        return discriminator_loss\n",
        "\n",
        "    def get_batch_data(self, data, n_windows):\n",
        "        data = tf.convert_to_tensor(data, dtype=tf.float32)\n",
        "        return iter(tf.data.Dataset.from_tensor_slices(data)\n",
        "                                .shuffle(buffer_size=n_windows)\n",
        "                                .batch(self.batch_size).repeat())\n",
        "\n",
        "    def _generate_noise(self):\n",
        "        while True:\n",
        "            yield np.random.uniform(low=0, high=1, size=(self.seq_len, self.n_seq))\n",
        "\n",
        "    def get_batch_noise(self):\n",
        "        return iter(tf.data.Dataset.from_generator(self._generate_noise, output_types=tf.float32)\n",
        "                                .batch(self.batch_size)\n",
        "                                .repeat())\n",
        "\n",
        "    def train(self, data, train_steps):\n",
        "        ## Embedding network training\n",
        "        autoencoder_opt = keras.optimizers.Adam(learning_rate=self.lr)\n",
        "        for _ in tqdm(range(train_steps), desc='Emddeding network training'):\n",
        "            X_ = next(self.get_batch_data(data, n_windows=len(data)))\n",
        "            step_e_loss_t0 = self.train_autoencoder(X_, autoencoder_opt)\n",
        "\n",
        "        ## Supervised Network training\n",
        "        supervisor_opt = keras.optimizers.Adam(learning_rate=self.lr)\n",
        "        for _ in tqdm(range(train_steps), desc='Supervised network training'):\n",
        "            X_ = next(self.get_batch_data(data, n_windows=len(data)))\n",
        "            step_g_loss_s = self.train_supervisor(X_, supervisor_opt)\n",
        "\n",
        "        ## Joint training\n",
        "        generator_opt = keras.optimizers.Adam(learning_rate=self.lr)\n",
        "        embedder_opt = keras.optimizers.Adam(learning_rate=self.lr)\n",
        "        discriminator_opt = keras.optimizers.Adam(learning_rate=self.lr)\n",
        "\n",
        "        step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = 0\n",
        "        for _ in tqdm(range(train_steps), desc='Joint networks training'):\n",
        "\n",
        "            #Train the generator (k times as often as the discriminator)\n",
        "            # Here k=2\n",
        "            for _ in range(2):\n",
        "                X_ = next(self.get_batch_data(data, n_windows=len(data)))\n",
        "                Z_ = next(self.get_batch_noise())\n",
        "\n",
        "                # Train the generator\n",
        "                step_g_loss_u, step_g_loss_s, step_g_loss_v = self.train_generator(X_, Z_, generator_opt)\n",
        "\n",
        "                # Train the embedder\n",
        "                step_e_loss_t0 = self.train_embedder(X_, embedder_opt)\n",
        "\n",
        "            X_ = next(self.get_batch_data(data, n_windows=len(data)))\n",
        "            Z_ = next(self.get_batch_noise())\n",
        "            step_d_loss = self.discriminator_loss(X_, Z_)\n",
        "            if step_d_loss > 0.15:\n",
        "                step_d_loss = self.train_discriminator(X_, Z_, discriminator_opt)\n",
        "\n",
        "    def sample(self, n_samples):\n",
        "        steps = n_samples // self.batch_size + 1\n",
        "        data = []\n",
        "        for _ in trange(steps, desc='Synthetic data generation'):\n",
        "            Z_ = next(self.get_batch_noise())\n",
        "            records = self.generator(Z_)\n",
        "            data.append(records)\n",
        "        return np.array(np.vstack(data))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USLChlfA8GR8"
      },
      "source": [
        "## Experiment\n",
        "The data is the googles stock downloaded from Yahoo Finance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdLeyfUE9Blq"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "import requests as req\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from google.colab import files"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tujAAQn9D0h"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X20yda29HQf"
      },
      "source": [
        "def real_data_loading(data: np.array, seq_len):\n",
        "    # Flip the data to make chronological data\n",
        "    ori_data = data[::-1]\n",
        "    # Normalize the data\n",
        "    scaler = MinMaxScaler().fit(ori_data)\n",
        "    ori_data = scaler.transform(ori_data)\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    temp_data = []\n",
        "    # Cut data by sequence length\n",
        "    for i in range(0, len(ori_data) - seq_len):\n",
        "        _x = ori_data[i:i + seq_len]\n",
        "        temp_data.append(_x)\n",
        "\n",
        "    # Mix the datasets\n",
        "    idx = np.random.permutation(len(temp_data))\n",
        "    data = []\n",
        "    for i in range(len(temp_data)):\n",
        "        data.append(temp_data[idx[i]])\n",
        "    return data\n",
        "\n",
        "def get_data(seq_len: int, stock_symbol='AMZN'):\n",
        "    file_path = os.path.join('/resources', 'data', f'{stock_symbol}.csv')\n",
        "    print(file_path)\n",
        "    try:\n",
        "        stock_df = pd.read_csv(file_path)\n",
        "    except:\n",
        "        if stock_symbol == 'GOOG':\n",
        "          stock_url = 'https://drive.google.com/u/0/uc?id=1psnECSivl3euQkL7SdX-ak6MI-SxVK8d&export=download'\n",
        "        elif stock_symbol == 'AMZN':\n",
        "          stock_url = 'https://drive.google.com/file/d/1SGOoKllIoT1kV97XegOz6vLPrLCO7eSa/view?usp=sharing'\n",
        "        elif stock_symbol == 'AAPL':\n",
        "          stock_url = 'https://drive.google.com/file/d/1k_b21HDfhdxeEyiSJ9Jp-CEYq2dNmOnV/view?usp=sharing'          \n",
        "        else:\n",
        "          raise Exception('Valid values for stock_symbol is GOOG and AMZN!')\n",
        "\n",
        "        request = req.get(stock_url)\n",
        "        url_content = request.content\n",
        "\n",
        "        if not os.path.exists(os.path.dirname(file_path)):\n",
        "            os.makedirs(os.path.dirname(file_path))\n",
        "        stock_csv = open(file_path, \"wb\")\n",
        "        stock_csv.write(url_content)\n",
        "        # Reading the stock data\n",
        "        stock_df = pd.read_csv(file_path)\n",
        "        print(stock_df.shape)\n",
        "    try:\n",
        "        stock_df = stock_df.set_index('Date').sort_index()\n",
        "    except:\n",
        "        stock_df=stock_df\n",
        "    #Data transformations to be applied prior to be used with the synthesizer model\n",
        "    processed_data = real_data_loading(stock_df.values, seq_len=seq_len)\n",
        "\n",
        "    return processed_data, stock_df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC4tlmBk_nXH"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htpVQsmz9Taq",
        "outputId": "5966f545-dd0c-410f-84ae-b80038e72b1c"
      },
      "source": [
        "seq_len = 24\n",
        "n_seq = 6\n",
        "hidden_dim = 24\n",
        "gamma = 1\n",
        "noise_dim = 32\n",
        "dim = 128\n",
        "batch_size = 128\n",
        "log_step = 100\n",
        "learning_rate = 5e-4\n",
        "gan_args = [batch_size, learning_rate, noise_dim, 24, 2, (0, 1), dim]\n",
        "\n",
        "stock_data, stock_df = get_data(seq_len=seq_len, stock_symbol='GOOG')\n",
        "print(len(stock_data), stock_data[0].shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/resources/data/GOOG.csv\n",
            "(1647, 7)\n",
            "1623 (24, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TEsKYsR_1-F"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sW2x-dHUtYK1",
        "outputId": "6e3aa9d7-893f-43f3-fc18-30f23c0d9d72"
      },
      "source": [
        "if os.path.exists(os.path.join('/resources', 'model', 'synthesizer_stock.pkl')):\n",
        "    synthysizer = TimeGAN.load(os.path.join('/resources', 'model', 'synthesizer_stock.pkl'))\n",
        "else:\n",
        "    synthysizer = TimeGAN(model_parameters=gan_args, hidden_dim=24, seq_len=seq_len, n_seq=n_seq, gamma=1)\n",
        "    synthysizer.train(stock_data, train_steps=50000)\n",
        "    # synthysizer.save(os.path.join(os.path.dirname(__file__), 'model', 'synthesizer_stock.pkl'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emddeding network training:  41%|████      | 20333/50000 [26:32<40:23, 12.24it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL-eK4mc_5xl"
      },
      "source": [
        "### Generate Synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ9-Cy8UtvAc"
      },
      "source": [
        "synthetic_data = synthysizer.sample(len(stock_data))\n",
        "print(synthetic_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo_WvPJ-__YE"
      },
      "source": [
        "### Plot to compare generated samples with real data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYuj27xZzRJs"
      },
      "source": [
        "!mkdir /resources/graphs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imoSb6m8t2jw"
      },
      "source": [
        "# Reshaping the data\n",
        "cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
        "\n",
        "# Plotting some generated samples. Both Synthetic and Original data are still standartized with values between [0,1]\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "time = list(range(1, 25))\n",
        "obs = np.random.randint(len(stock_data))\n",
        "\n",
        "for j, col in enumerate(cols):\n",
        "    df = pd.DataFrame({'Real': stock_data[obs][:, j],\n",
        "                        'Synthetic': synthetic_data[obs][:, j]})\n",
        "    df.plot(ax=axes[j],\n",
        "            title=col,\n",
        "            secondary_y='Synthetic data', style=['-', '--'])\n",
        "\n",
        "    try:\n",
        "        df.to_csv(os.path.join('/resources', 'data', f'synthesized_stock_df_{col}.csv'))\n",
        "    except Exception as Err:\n",
        "        print('Saving synthesized_stock failed!', str(Err))\n",
        "        pass\n",
        "fig.tight_layout()\n",
        "plt.savefig(os.path.join('/resources', 'graphs', 'data_comparison.png'))\n",
        "files.download(os.path.join('/resources', 'graphs', 'data_comparison.png')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhFR2LJkt4_C"
      },
      "source": [
        "sample_size = 250\n",
        "idx = np.random.permutation(len(stock_data))[:sample_size]\n",
        "\n",
        "real_sample = np.asarray(stock_data)[idx]\n",
        "synthetic_sample = np.asarray(synthetic_data)[idx]\n",
        "\n",
        "# We use only two componentes for both the PCA and TSNE for comparision in 2-Dimensional space.\n",
        "reduced_synth_data = real_sample.reshape(-1, seq_len)\n",
        "reduced_stock_data = np.asarray(synthetic_sample).reshape(-1, seq_len)\n",
        "\n",
        "n_components = 2\n",
        "pca = PCA(n_components=n_components)\n",
        "tsne = TSNE(n_components=n_components, n_iter=300)\n",
        "\n",
        "# The fit of the methods must be done only using the real sequential data\n",
        "pca.fit(reduced_stock_data)\n",
        "\n",
        "pca_real = pd.DataFrame(pca.transform(reduced_stock_data))\n",
        "pca_synth = pd.DataFrame(pca.transform(reduced_synth_data))\n",
        "\n",
        "reduced_data = np.concatenate((reduced_stock_data, reduced_synth_data), axis=0)\n",
        "tsne_results = pd.DataFrame(tsne.fit_transform(reduced_data))\n",
        "\n",
        "# The scatter plots for PCA and TSNE methods\n",
        "fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
        "spec = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n",
        "\n",
        "# TSNE scatter plot\n",
        "ax = fig.add_subplot(spec[0, 0])\n",
        "ax.set_title('PCA results',\n",
        "              fontsize=20,\n",
        "              color='red',\n",
        "              pad=10)\n",
        "\n",
        "# PCA scatter plot\n",
        "plt.scatter(pca_real.iloc[:, 0].values, pca_real.iloc[:, 1].values,\n",
        "            c='black', alpha=0.2, label='Original')\n",
        "plt.scatter(pca_synth.iloc[:, 0], pca_synth.iloc[:, 1],\n",
        "            c='red', alpha=0.2, label='Synthetic')\n",
        "ax.legend()\n",
        "\n",
        "ax2 = fig.add_subplot(spec[0, 1])\n",
        "ax2.set_title('TSNE results',\n",
        "              fontsize=20,\n",
        "              color='red',\n",
        "              pad=10)\n",
        "\n",
        "plt.scatter(tsne_results.iloc[:sample_size, 0].values, tsne_results.iloc[:sample_size, 1].values,\n",
        "            c='black', alpha=0.2, label='Original')\n",
        "plt.scatter(tsne_results.iloc[sample_size:, 0], tsne_results.iloc[sample_size:, 1],\n",
        "            c='red', alpha=0.2, label='Synthetic')\n",
        "\n",
        "ax2.legend()\n",
        "\n",
        "fig.suptitle('Validating synthetic vs real data diversity and distributions',\n",
        "              fontsize=16,\n",
        "              color='grey')\n",
        "\n",
        "# fig.tight_layout()\n",
        "plt.savefig(os.path.join('/resources', 'graphs', 'synthetic_vs_real.png'))\n",
        "files.download(os.path.join('/resources', 'graphs', 'synthetic_vs_real.png')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVFUjtel6-hw"
      },
      "source": [
        "### Train a model on Synthetic data and test on real data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gp9-ZDDyg-c"
      },
      "source": [
        "def simple_RNN_regressor(units):\n",
        "    opt = keras.optimizers.Adam(name='AdamOpt')\n",
        "    loss = keras.losses.MeanAbsoluteError(name='MAE')\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.GRU(units=units,\n",
        "                  name=f'RNN_1'))\n",
        "    model.add(keras.layers.Dense(units=6,\n",
        "                    activation='sigmoid',\n",
        "                    name='OUT'))\n",
        "    model.compile(optimizer=opt, loss=loss)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVqPxol-y9v7"
      },
      "source": [
        "#Prepare the dataset for the regression model\n",
        "stock_data=np.asarray(stock_data)\n",
        "synth_data = synth_data[:len(stock_data)]\n",
        "n_events = len(stock_data)\n",
        "\n",
        "#Split data on train and test\n",
        "idx = np.arange(n_events)\n",
        "n_train = int(.75*n_events)\n",
        "train_idx = idx[:n_train]\n",
        "test_idx = idx[n_train:]\n",
        "\n",
        "#Define the X for synthetic and real data\n",
        "X_stock_train = stock_data[train_idx, :seq_len-1, :]\n",
        "X_synth_train = synth_data[train_idx, :seq_len-1, :]\n",
        "\n",
        "X_stock_test = stock_data[test_idx, :seq_len-1, :]\n",
        "y_stock_test = stock_data[test_idx, -1, :]\n",
        "\n",
        "#Define the y for synthetic and real datasets\n",
        "y_stock_train = stock_data[train_idx, -1, :]\n",
        "y_synth_train = synth_data[train_idx, -1, :]\n",
        "\n",
        "print('Synthetic X train: {}'.format(X_synth_train.shape))\n",
        "print('Real X train: {}'.format(X_stock_train.shape))\n",
        "\n",
        "print('Synthetic y train: {}'.format(y_synth_train.shape))\n",
        "print('Real y train: {}'.format(y_stock_train.shape))\n",
        "\n",
        "print('Real X test: {}'.format(X_stock_test.shape))\n",
        "print('Real y test: {}'.format(y_stock_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdDaolyD8aXV"
      },
      "source": [
        "#### Train on real data and test on real data \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXyjUw-V71Sb"
      },
      "source": [
        "#Training the model with the real train data\n",
        "ts_real = simple_RNN_regressor(12)\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss')\n",
        "\n",
        "real_train = ts_real.fit(x=X_stock_train,\n",
        "                          y=y_stock_train,\n",
        "                          validation_data=(X_stock_test, y_stock_test),\n",
        "                          epochs=200,\n",
        "                          batch_size=128,\n",
        "                          callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADn3u4MS8iaZ"
      },
      "source": [
        "#### Train on Synthetic data test on real data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2anaK02V71G9"
      },
      "source": [
        "#Training the model with the synthetic data\n",
        "ts_synth = simple_RNN_regressor(12)\n",
        "synth_train = ts_synth.fit(x=X_synth_train,\n",
        "                          y=y_synth_train,\n",
        "                          validation_data=(X_stock_test, y_stock_test),\n",
        "                          epochs=200,\n",
        "                          batch_size=128,\n",
        "                          callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHeEV_aK8vfZ"
      },
      "source": [
        "#### Summeries metrics in a dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZrXqz4U71D8"
      },
      "source": [
        "#Summarize the metrics here as a pandas dataframe\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_log_error\n",
        "real_predictions = ts_real.predict(X_stock_test)\n",
        "synth_predictions = ts_synth.predict(X_stock_test)\n",
        "\n",
        "metrics_dict = {'r2': [r2_score(y_stock_test, real_predictions),\n",
        "                       r2_score(y_stock_test, synth_predictions)],\n",
        "                'MAE': [mean_absolute_error(y_stock_test, real_predictions),\n",
        "                        mean_absolute_error(y_stock_test, synth_predictions)],\n",
        "                'MRLE': [mean_squared_log_error(y_stock_test, real_predictions),\n",
        "                         mean_squared_log_error(y_stock_test, synth_predictions)]}\n",
        "\n",
        "results = pd.DataFrame(metrics_dict, index=['Real', 'Synthetic'])\n",
        "\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EXWZQnC71AF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}